---
title: "Notebbok"
author: "Abdullah Abdelaziz"
format: html
editor: visual
---


# Packages
```{r}
pacman::p_load(tidyverse, modelsummary, gtsummary, marginaleffects,
               rio, broom, easystats, rsample, fixest)
```

# Datasets
https://github.com/proback/BeyondMLR

Note: You need to replace `github.com` with `raw.githubusercontent.com` to read the data not the display version of the data

# Chapter 1

```{r}
#|label: read data
derbyplus <- read.csv(url("https://raw.githubusercontent.com/abduazizR/BeyondMLR/master/data/derbyplus.csv"))
```

```{r}
#| label: clean data

derbyplus <- derbyplus |> 
  mutate(fast = ifelse(condition == "fast", 1,0),
         good = ifelse(condition == "good", 1,0),
         yearnew = year - 1896,
         fastfactor = case_when(
           fast == 0 ~ "not fast",
           fast == 1 ~ "fast"
         ) |> fct_relevel(c("not fast", "fast")))
```

```{r}
#| label: modeling

# No centring for year
lm(speed ~ year, 
   data = derbyplus) |> 
  modelsummary()

# Centering year
linear_model <- lm(speed ~ yearnew, 
   data = derbyplus)

quadratic_model <- lm(speed ~ poly(yearnew, 2, raw = T), 
   data = derbyplus)

# How to plot different models on a plot
derbyplus |> 
  ggplot(aes(x = yearnew, y = speed)) +geom_point()  +
  geom_line(data = augment(quadratic_model, data = derbyplus),
              aes(x = yearnew, y = .fitted), color = "blue") +
  geom_line(data = augment(linear_model, data = derbyplus),
              aes(x = yearnew, y = .fitted), color = "red")  
```

```{r}
#| label: model diagnostics

# No centring for year
lm(speed ~ year, 
   data = derbyplus) |> 
  check_model()

# Centering year
lm(speed ~ yearnew, 
   data = derbyplus) |> 
  check_model()

quadratic_model |> summary()

quadratic_model |> check_model()
```

```{r}
derbyplus |> 
  bootstraps(100) |> 
  pull(splits)  %>% 
  map_dfr(~lm(speed ~ yearnew + fast, data = .) %>% 
            tidy())%>% 
  group_by(term) %>% 
  dplyr::summarize(low=quantile(estimate, .025),
            high=quantile(estimate, .975))
```

# Chapter 2

```{r}
Composition <- c("B","G","BB","BG","GB","GGB","GBB","Total")
NumbFams <- c(6,7,5,4,5,1,2,30)
NumbChild <- c(6,7,10,8,10,3,6,50)


table1chp2 <- data.frame(Composition, NumbFams, NumbChild)
colnames(table1chp2) <- c("Composition","Number of families", "Number of children")

table1chp2
# Seeing likelihoods graphically
pb <- seq(0,1, length = 10000)
lik <- pb^5416*(1-pb)^5256

tibble(
  pb, lik
) %>%
  ggplot(aes(x = pb, y = lik)) + geom_line()
```



```{r}
oLik.f <- function(pb){
    return(5416*log(pb) + 5256*log(1-pb))
  }
optimize(oLik.f, interval=c(0,1), maximum=TRUE)
```

# Chapter 3

```{r}
# binomial distribution
plotbinomial <- function(n,p){
  y1 <- 0:n # all possible success from the n trials
  prob <- dbinom(y1, size = n, prob = p)
  BBGdf <- tibble(y1, prob)
  ggplot(data = BBGdf, aes(x = y1, xend = y1, y = 0, yend = prob)) + geom_segment()
  
}

plotbinomial(n = 100, p = 0.8)
```


```{r}
plot_geometric <- function(p, n){
  n_failures <- 0:n
  prob <- dgeom(n_failures, prob = p)
  BBGdf <- tibble(n_failures, prob)
  ggplot(data = BBGdf, aes(x = n_failures, xend = n_failures, y = 0, yend = prob)) + geom_segment()
  
}

plot_geometric(n = 15, p = 0.2)

rgeom(100, 0.3) |> 
  tibble() |> 
  ggplot(aes(x = `rgeom(100, 0.3)`)) + geom_histogram()
```
```{r}
# Beta binomial distribution
x <- rbinom(1000, 10, 0.8)

x |> 
  tibble() |> 
  ggplot(aes(x)) + geom_histogram()


x <- rbeta(1, shape1 = 4, shape2 = 1)

y_vec = c()
for (i in 1:1000) {
  x = rbeta(1, shape1 = 4, shape2 = 1)
  y <- rbinom(1, 10, x)
  y_vec <- c(y_vec,y)
  i = i+1
}

y_vec |> 
  tibble() |> 
  ggplot(aes(x = y_vec)) + geom_histogram()
```

```{r}
#|label: Gamma Poisson mixutre

# Vanilla Poisson
vanilla_poisson <- rpois(10000, 1.5)
hist(vanilla_poisson)
mean(vanilla_poisson)
sd(vanilla_poisson)


# Vanilla Gamma
vanilla_gamma <- rgamma(10000, 3, 2)
hist(vanilla_gamma)
mean(vanilla_gamma)
sd(vanilla_gamma)


gamma_pois = c()
for (i in 1:10000) {
  x = rgamma(1, 3, 2)
  y <- rpois(1, x)
  gamma_pois <- c(gamma_pois,y)
  i = i+1
}
gamma_pois |> 
  tibble() |> 
  ggplot(aes(x = gamma_pois)) + geom_histogram()
mean(gamma_pois)
sd(gamma_pois)
```

```{r}
# Gamma-Poisson mixture and negative binomial
gamma_pois = c()
for (i in 1:10000) {
  x = rgamma(1, 3, 2)
  y <- rpois(1, x)
  gamma_pois <- c(gamma_pois,y)
  i = i+1
}
gamma_pois |> 
  tibble() |> 
  ggplot(aes(x = gamma_pois)) + geom_histogram()
gamma_pois|> tibble() |> rstatix::get_summary_stats()

neg_binom <- rnbinom(10000, mu = 1.5, size = 3)


neg_binom |> 
  tibble() |> 
  ggplot(aes(x = neg_binom)) + geom_histogram()
neg_binom |> tibble() |> rstatix::get_summary_stats()
```


```{r}
log_likelihood <- function(params){
  mu_1 <- params[1]
  sigma_1 <- params[2]
  mu_2 <- params[3]
  sigma_2 <- params[4]
  alpha <- params[5]
  
  pdf1 <- dnorm(faithful$waiting, mean = mu_1, sd = sigma_1)
  pdf2 <- dnorm(faithful$waiting, mean = mu_2, sd = sigma_2)
  
  mixture_pdf <- alpha * pdf1 + (1 - alpha) * pdf2
  
    log_likelihood <- sum(log(mixture_pdf))
  
  return(log_likelihood)
}

# Set initial parameter values
initial_params <- c(3, 1, 70, 10, 0.5)

# Define the negative log-likelihood function (since optim() minimizes by default)
negative_log_likelihood <- function(params) {
  return(-log_likelihood(params))
}

# Find the MLEs
result <- optim(initial_params, negative_log_likelihood)
```

# Chapter 4 Poisson regression

```{r}
#| label: Households in Philppines data

fHH1 <- read.csv(url("https://raw.githubusercontent.com/abduazizR/BeyondMLR/master/data/fHH1.csv"))
```



```{r}
fHH1 |> 
  gtsummary::tbl_summary()

```

## How to empirically check for the Poisson assumption mean equals variance
Pick an exaplanatory variable say age in our example.
Discretize it in a number of starata.
Calculate the mean and the variance of the outcome within each strata.
If they are equal, this data is good for Poisson model


```{r}
fHH1 |> 
  mutate(age_group = cut_interval(age, length = 5)) |> 
  summarise(.by = age_group,
            mean_outcome = mean(total),
            var_outcome = var(total),
            n = n()) |> 
  arrange(age_group)
```

## How to empirically check for log(outcome) is linearly related to the exposure

```{r}
fHH1 |> 
  group_by(age) |> 
  summarise(log_mean_total = log(mean(total))) |> 
  ggplot(aes(x = age, y = log_mean_total)) + geom_point() + geom_smooth()

# by region
fHH1 |> 
  group_by(age, location) |> 
  summarise(log_mean_total = log(mean(total))) |> 
  ggplot(aes(x = age, y = log_mean_total, group = location)) + geom_point(aes(shape = location, color = location)) + geom_smooth(se = F, aes(linetype = location, color = location))
```

# Fit a Poisson model

